<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>samirparhi.com - Meshery</title>
    <subtitle>Blog Site of Samir Parhi</subtitle>
    <link rel="self" type="application/atom+xml" href="https://samirparhi.com/tags/meshery/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://samirparhi.com"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2024-07-04T00:00:00+00:00</updated>
    <id>https://samirparhi.com/tags/meshery/atom.xml</id>
    <entry xml:lang="en">
        <title>Creating An Aks Private cluster with Istio and Application Gateway</title>
        <published>2024-07-04T00:00:00+00:00</published>
        <updated>2024-07-04T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Samir Ranjan Parhi
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://samirparhi.com/creating-a-aks-private-cluster-with-application-gateway-and-istio/"/>
        <id>https://samirparhi.com/creating-a-aks-private-cluster-with-application-gateway-and-istio/</id>
        
        <content type="html" xml:base="https://samirparhi.com/creating-a-aks-private-cluster-with-application-gateway-and-istio/">&lt;p&gt;Security has been a very key consideration in modern application architecture. When we talk about cloud-native application architecture, people tend to think about security a lot. That is why all the cloud providers have a bunch of features to make your application more secure and robust. For example, Azure provides various gateways like Network security groups, Application gateway, Azure front-door. These azure services provide a secure connection either by introducing a firewall or validating SSL certificates.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;people say ‚ÄúSecurity is a myth over the internet&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Among all the various way of the application deployment strategy, Most of the organisation has adopted or are adopting to containerise their application and run in a Kubernetes cluster. This adaptation is influenced by the features and the advantages provided by Kubernetes. Due to this popularity, all the cloud providers had also introduced the managed Kubernetes clusters. In the case of Azure, it is known as Azure Kubernetes services (aka AKS).&lt;&#x2F;p&gt;
&lt;p&gt;In this article, we will talk about how we can create a more secure infrastructure with the azure Kubernetes services and azure application gateway. It has all the information and detailed explanations to get your cluster bootstrapped and start serving your production workload. Let‚Äôs get started.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;prerequisite&quot;&gt;Prerequisite&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;You should have a valid subscription and proper access in Azure to create an AKS service.
Essentially, it better to have a dedicated resource group created for the AKS and you should have the owner permission to that resource group.&lt;&#x2F;li&gt;
&lt;li&gt;Understanding of Azure private DNS zone.&lt;&#x2F;li&gt;
&lt;li&gt;Understanding of Istio service mesh. In case you don‚Äôt know how to deploy, make use of Meshery (https:&#x2F;&#x2F;meshery.io&#x2F;) which will install and configure the service mesh for you.&lt;&#x2F;li&gt;
&lt;li&gt;Should know various K8s concepts essentially the POD, Services, Deployment, Namespace, Ingress Controller, and ingress gateway. Helm and kubectl should be installed.&lt;&#x2F;li&gt;
&lt;li&gt;Idea about the CIDR block.&lt;&#x2F;li&gt;
&lt;li&gt;Of course azure-cli should be installed and have an Idea to run az Command.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h4 id=&quot;networking-concept-in-aks&quot;&gt;Networking Concept in AKS:&lt;&#x2F;h4&gt;
&lt;p&gt;Before getting into the configuration, let‚Äôs understand the concept of networking used in the azure Kubernetes cluster which with ease our process of setting up the private cluster. Azure provides two kinds of networking in AKS namely kubenet and azure CNI.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kubenet :&lt;&#x2F;strong&gt;  This is the default networking being used by AKS cluster. This by default takes care of all the networking behind the scene.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: All AKS nodes get an IP address from the Azure virtual network subnet. Pods receive an IP address from a logically different address space to the Azure virtual network subnet of the nodes. Network address translation (NAT) is then configured so that the pods can reach resources on the Azure virtual network.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Azure CNI:&lt;&#x2F;strong&gt; This is also termed advance networking. If you choose to go with the CNI route you have to do some more homework but the beauty is you can customize the networking the way you want. We will be using CNI in our context.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Note:Every pod gets an IP address from the subnet and can be accessed directly. These IP addresses must be unique across your network space, and must be planned in advance. Each node has a configuration parameter for the maximum number of pods that it supports. The equivalent number of IP addresses per node is then reserved upfront for that node. You can configure the maximum pods deployable to a node at cluster create time or when creating new node pools. If you don‚Äôt specify maxPods when creating new node pools, you receive a default value of 110 for kubenet.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;ip-planning&quot;&gt;IP planning&lt;&#x2F;h3&gt;
&lt;p&gt;As discussed, Azure CNI assigns all the real IP to both Nodes and the pods from a given subnet space, So the Subnet should be capable enough to hold these IP ranges
The calculation below explains the min requirement of no IPs.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Assumption :&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;‚Ä¢ We are creating a 3 node cluster - Means 3 IPs will be taken from the subnet assigned to K8s.&lt;&#x2F;p&gt;
&lt;p&gt;‚Ä¢ By default each node can have a max of 30 Pods i.e., 30*3=90 IP should be taken from the same subnet.&lt;&#x2F;p&gt;
&lt;p&gt;‚Ä¢ From a subnet, azure reserves 5 IP addresses for its internal use.&lt;&#x2F;p&gt;
&lt;p&gt;‚Ä¢ One extra node to be considered for the rolling update which will occupy 31 IPs (considering 30 pods per node by default)&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Maximum Pod per node are limited to 250&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;By considering the above scenario, the subnet, where we want to create your K8s cluster, should have 90+3+5+31=129 IPs As our intention is to create the AKS Private cluster let‚Äôs see the private address spaces that we can use.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚úçüèºTip: Below table shows Private address ranges available for consumption&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;cidrs&quot;&gt;CIDRs :&lt;&#x2F;h3&gt;
&lt;p&gt;Let‚Äôs Design our CIDR for various network requirements of AKS cluster via Azure CNI Approach.   Vnet CIDR: 192.168.0.0&#x2F;20 - total 4,096 Ips (192.168.0.0 to 192.168.15.255)   AKS primary Subnet CIDR: 192.168.0.0&#x2F;21 - total 2043 IPs (192.168.0.0 - 192.168.7.255)&lt;&#x2F;p&gt;
&lt;h4 id=&quot;terminology-used-in-azure-cni&quot;&gt;Terminology Used in Azure CNI:&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;strong&gt;Virtual network:&lt;&#x2F;strong&gt;  This is the virtual network, where you want to create your K8S Cluster. Here, I have taken my Vnet range as &lt;code&gt;192.168.0.0&#x2F;20&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Cluster Subnet:&lt;&#x2F;strong&gt; This is the Subnet inside which your K8S cluster will be created. Please note that you Should create the subnet prior to create the AKS cluster and, it should have sufficient address space. The address space requirement explained at the beginning of this document. For example, I am choosing &lt;code&gt;192.168.0.0&#x2F;21&lt;&#x2F;code&gt; range as my subnet for the cluster.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Kubernetes service address range:&lt;&#x2F;strong&gt; This is the  CIDR or IP range that is the K8S services will be using. Please note that this IP range must not be part of any of the subnets in your cluster. In simple words, it should be different from your subnet IP ranges. Important to note that this IP range should be in the range of your VNET IP range.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, this address range can not be in 192.168.0.0&#x2F;21 range, as my cluster subnet is using it, but it should be in 192.168.0.0&#x2F;20 range. So I am choosing 192.168.8.0&#x2F;22 (total 1024 IPs (192.168.8.0 - 192.168.11.255))&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kubernetes DNS service IP address:&lt;&#x2F;strong&gt; This is a Distinct IP address from your Kubernetes service address range. Please note that this should be the first and last IP address of your Kubernetes service address range.   For example,  in Kubernetes service address range is &lt;code&gt;192.168.8.0&#x2F;22&lt;&#x2F;code&gt;. Kubernetes DNS service IP address can not be &lt;code&gt;192.168.8.0&lt;&#x2F;code&gt; or &lt;code&gt;192.168.11.255&lt;&#x2F;code&gt;. So I am choosing &lt;code&gt;192.168.8.8&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Docker Bridge address: this is the address range that will be used for docker bridge networking.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Few things to remember :&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;This range should not be in the range of the Kubernetes service address range. In this case &lt;code&gt;192.168.8.0&#x2F;22&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Addresses matching the format &lt;code&gt;...0 or ...255&lt;&#x2F;code&gt; are reserved and cannot be used.&lt;&#x2F;li&gt;
&lt;li&gt;The prefix must be between 1 and 29.&lt;&#x2F;li&gt;
&lt;li&gt;The IP address cannot be the first or last address in its CIDR block In my scenario, I am choosing &lt;code&gt;192.168.12.2&#x2F;28&lt;&#x2F;code&gt;(total 16 IPs (&lt;code&gt;192.168.12.0&lt;&#x2F;code&gt; - &lt;code&gt;192.168.12.15&lt;&#x2F;code&gt;)), which satisfies all the above conditions.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;DNS name prefix:&lt;&#x2F;strong&gt; This is the DNS name that will be resolved for the API server of the Kubernetes cluster. You will use this FQDN to access your API server after you host your AKS cluster and the application.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;what-is-azure-application-gateway&quot;&gt;What is Azure Application Gateway?&lt;&#x2F;h4&gt;
&lt;p&gt;Azure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications. This also provides the firewall capability to secure the traffic to your application. Below image from Microsoft documentation.&lt;&#x2F;p&gt;
&lt;h5 id=&quot;our-solution-architecture-diagram&quot;&gt;Our Solution Architecture Diagram:&lt;&#x2F;h5&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;samirparhi-dev&#x2F;samirparhi-dev&#x2F;main&#x2F;blog&#x2F;sol-arch-azure.png&quot; alt=&quot;Our Architechture&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Explanation:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;When users request for the website, it reaches the name server from where you have purchased your DNS (it can be azure or any 3rd party). From the name server, it finds A record and directed to the public IP of the Azure application gateway. When the request reaches the Application gateway, it validates the SSL certificate and runs the firewall rules too. then the Rules in the application gateway redirect it to its proper backend. The backend-pool for the application gateway is the istio ingress gateway. As we are constructing a fully private cluster (meaning all the IPs in the AKS cluster are private IPs) we have used DNS private Zone to implement a trusted connection between all the azure services in our VNET.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;put-things-to-action&quot;&gt;Put things to Action:&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;step-1&quot;&gt;Step 1:&lt;&#x2F;h4&gt;
&lt;p&gt;Create a resource group name k8s-rg in centralindia location (make sure to set the subscription first( here I am using my subscription named Azure)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;az account set --subscription &quot;Azure&quot; &amp;amp;&amp;amp; az group create -l centralindia -n k8s-rg&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;step-2-create-vnet-and-subnet-in-the-above-created-resource-group&quot;&gt;Step 2: Create vnet and subnet in the above-created resource group.&lt;&#x2F;h4&gt;
&lt;pre style=&quot;background-color:#212121;color:#eeffff;&quot;&gt;&lt;code&gt;&lt;span&gt;az network vnet create -g k8s-rg -n k8s-vnet --address-prefix 192.168.0.0&#x2F;20 \
&lt;&#x2F;span&gt;&lt;span&gt;--subnet-name k8s-subnet --subnet-prefix 192.168.0.0&#x2F;21
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: I have used the CIDR that is discussed above. Please refer earlier section.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h4 id=&quot;step-3-now-let-s-get-the-id-of-the-subnet-we-created-earlier&quot;&gt;Step 3: Now let‚Äôs get the ID of the subnet we created earlier&lt;&#x2F;h4&gt;
&lt;pre style=&quot;background-color:#212121;color:#eeffff;&quot;&gt;&lt;code&gt;&lt;span&gt;    az network vnet subnet list \     
&lt;&#x2F;span&gt;&lt;span&gt;--resource-group k8s-rg \     
&lt;&#x2F;span&gt;&lt;span&gt;--vnet-name k8s-vnet \     
&lt;&#x2F;span&gt;&lt;span&gt;--query &amp;quot;[0].id&amp;quot; --output tsv
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;you will get the below output, save it for use in the next step:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;&#x2F;subscriptions&#x2F;&amp;lt;subscriptionID&amp;gt;&#x2F;resourceGroups&#x2F;rg-k8s&#x2F;providers&#x2F;Microsoft.Network&#x2F;virtualNetworks&#x2F;k8s-vnet&#x2F;subnets&#x2F;k8s-subnet&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;step-4-let-s-create-our-aks-cluster&quot;&gt;Step 4. Let‚Äôs create our AKS cluster:&lt;&#x2F;h4&gt;
&lt;pre style=&quot;background-color:#212121;color:#eeffff;&quot;&gt;&lt;code&gt;&lt;span&gt;az aks create \
&lt;&#x2F;span&gt;&lt;span&gt;    --resource-group k8s-rg \
&lt;&#x2F;span&gt;&lt;span&gt;    --name k8s-private \
&lt;&#x2F;span&gt;&lt;span&gt;    --network-plugin azure \
&lt;&#x2F;span&gt;&lt;span&gt;    --vnet-subnet-id &#x2F;subscriptions&#x2F;&amp;lt;subscriptionID&amp;gt;&#x2F;resourceGroups&#x2F;rg-k8s&#x2F;providers&#x2F;Microsoft.Network&#x2F;virtualNetworks&#x2F;k8s-vnet&#x2F;subnets&#x2F;k8s-subnet \
&lt;&#x2F;span&gt;&lt;span&gt;    --docker-bridge-address 192.168.12.2&#x2F;28 \
&lt;&#x2F;span&gt;&lt;span&gt;    --dns-service-ip 192.168.8.8 \
&lt;&#x2F;span&gt;&lt;span&gt;    --service-cidr 192.168.8.0&#x2F;22 \
&lt;&#x2F;span&gt;&lt;span&gt;    --generate-ssh-keys
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;step-5-add-node-pool-to-aks-cluster&quot;&gt;Step 5. add node pool to AKS Cluster:&lt;&#x2F;h4&gt;
&lt;pre style=&quot;background-color:#212121;color:#eeffff;&quot;&gt;&lt;code&gt;&lt;span&gt;az aks nodepool add \
&lt;&#x2F;span&gt;&lt;span&gt;    --cluster-name k8s-private \
&lt;&#x2F;span&gt;&lt;span&gt;    --resource-group \
&lt;&#x2F;span&gt;&lt;span&gt;    --name k8s-pool \ 
&lt;&#x2F;span&gt;&lt;span&gt;    --vnet-subnet-id &#x2F;subscriptions&#x2F;&amp;lt;subscriptionID&amp;gt;&#x2F;resourceGroups&#x2F;rg-k8s&#x2F;providers&#x2F;Microsoft.Network&#x2F;virtualNetworks&#x2F;k8s-vnet&#x2F;subnets&#x2F;k8s-subnet
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre style=&quot;background-color:#212121;color:#eeffff;&quot;&gt;&lt;code&gt;&lt;span&gt;az aks get-credentials --resource-group k8s-rg --name k8s-private \
&lt;&#x2F;span&gt;&lt;span&gt;kubectl get nodes
&lt;&#x2F;span&gt;&lt;span&gt;NAME                               STATUS   ROLES   AGE     VERSION
&lt;&#x2F;span&gt;&lt;span&gt;aks-k8s-pool-63828758-vmss000002   Ready    agent   6d11h   v1.19.11
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now the AKS cluster is ready.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;step-7-install-istio-through-meshery&quot;&gt;Step 7. Install Istio through Meshery.&lt;&#x2F;h4&gt;
&lt;p&gt;Meshery is an open-source, service mesh management plane that enables the adoption, operation, and management of any service mesh and its workloads.&lt;&#x2F;p&gt;
&lt;p&gt;Install Meshery through Helm: https:&#x2F;&#x2F;meshery.io&#x2F;#getting-started (click on Helm icon to see the instruction to install meshery via helm)&lt;&#x2F;p&gt;
&lt;p&gt;Install Istio via from meshey UI: https:&#x2F;&#x2F;docs.meshery.io&#x2F;service-meshes&#x2F;adapters&#x2F;istio&lt;&#x2F;p&gt;
&lt;p&gt;Step 8. Making the ingress gateway internal By default the ingress gateway ( in this context Istio Ingress gateway) are internet-facing, meaning they get a public IP. In this case, we are creating a private cluster which means the cluster should only be accessed from VNET internally. To achieve this we have to make our Ingress gateway IP a private IP. This can be done by adding an annotation to our ingress service manifest. the annotation is as follows :&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code class=&quot;text-sm&quot;&gt;
annotations:
    service.beta.kubernetes.io&#x2F;azure-load-balancer-internal: &quot;true&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;after applying the above annotation, check your ingress gateway IP now it is a private IP:&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code class=&quot;text-sm&quot;&gt;
kubectl get svc -n istio-system
NAME                   TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)                                      AGE
istio-ingressgateway   LoadBalancer   192.168.9.186   192.168.3.193    15021:30305&#x2F;TCP,80:32107&#x2F;TCP,443:32436&#x2F;TCP   6d10h
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;step-9&quot;&gt;Step 9:&lt;&#x2F;h4&gt;
&lt;p&gt;Now let‚Äôs create an azure private DNS zone and create a CNAME for our Istio Ingress gateway. So that the ingress gateway can be accessed via the azure network. More info on Azure private DNS (https:&#x2F;&#x2F;docs.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;dns&#x2F;private-dns-overview)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;az network private-dns zone create -g k8s-rg -n privatelink.centralindia.azmk8s.io&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;in above the private DNS name is an azure defined name for the k8s services more info (https:&#x2F;&#x2F;docs.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;private-link&#x2F;private-endpoint-dns)&lt;&#x2F;p&gt;
&lt;p&gt;now let‚Äôs add an A record for our istio ingress gateway so that it is discoverable by the application gateway in the Azure network. Go to the &lt;code&gt;privatelink.centralindia.azmk8s.io&lt;&#x2F;code&gt; private DNS resource that was created and click on recordset and add an ARecord as shown below and save it.&lt;&#x2F;p&gt;
&lt;p&gt;With this, we have successfully created a cluster and it is private. Now it can not receive any request from the web. But this is discoverable in the azure internal network as we have set up a private DNS zone. To enable the request from the internet let‚Äôs create an application gateway.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;step-10-to-create-an-application-gateway-you-need-to-create-a-public-ip-first&quot;&gt;Step 10. To create an application gateway, you need to create a Public IP first :&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;code&gt;az network public-ip create -g k8s-rg -n k8s-gw-pub-ip --allocation-method Static&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;h5 id=&quot;step-11-let-s-create-the-application-gateway-now-with-the-backend-pool-as-the-istio-ingress-gateway&quot;&gt;Step 11. Let‚Äôs create the application gateway now with the backend pool as the Istio ingress gateway.&lt;&#x2F;h5&gt;
&lt;pre&gt;&lt;code class=&quot;text-sm&quot;&gt;

az network application-gateway create \
  --name k8s-app-gateway \
  --location centralindia \
  --resource-group k8s-rg \
  --capacity 2 \
  --sku Standard_v2 \
  --public-ip-address k8s-gw-pub-ip \
  --vnet-name k8s-vnet \
  --subnet k8s-vnet \
  --servers sdistiogw.privatelink.centralindia.azmk8s.io
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;With this, we have successfully created our azure application gateway and also the backend is set to the istio ingress gateway. Now the application gateway will be able to forward traffic to the istio ingress gateway and also get the response from the Istio gateway.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;credits&quot;&gt;Credits:&lt;&#x2F;h3&gt;
&lt;pre style=&quot;background-color:#212121;color:#eeffff;&quot;&gt;&lt;code&gt;&lt;span&gt;‚Ä¢ Azure Docs
&lt;&#x2F;span&gt;&lt;span&gt;‚Ä¢ Istio
&lt;&#x2F;span&gt;&lt;span&gt;‚Ä¢ Meshery
&lt;&#x2F;span&gt;&lt;span&gt;‚Ä¢ Layer5
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;I hope you like this blog post. Please let me know your thoughts on it and suggest if you would like me to write on a specific topic.&lt;&#x2F;p&gt;
&lt;p&gt;Cheers !!!&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Configuring Meshery behind ISTIO and ingress gateway</title>
        <published>2024-05-06T00:00:00+00:00</published>
        <updated>2024-05-06T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Samir Ranjan Parhi
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://samirparhi.com/configuring-meshery-behind-istio-ingress-gateway/"/>
        <id>https://samirparhi.com/configuring-meshery-behind-istio-ingress-gateway/</id>
        
        <content type="html" xml:base="https://samirparhi.com/configuring-meshery-behind-istio-ingress-gateway/">&lt;p&gt;Accessing Workloads behind a ingress-gateway always has been a  industry standard practice in Kubernetes setup. It facilitate single  entry point for all your services deployed in a production grade  Kubernetes. This setup also allows you to leverage the service-mesh  functionality of implementing policies and have a better authz and authn  to the deployed services. Meshery is no different, you can configure it  to be accessed through ingress gateway. Let‚Äôs see how can we configure  it.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;prerequisite&quot;&gt;Prerequisite :&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes is up and running.&lt;&#x2F;li&gt;
&lt;li&gt;Ingress controller is installed and Ingress-gateway is provisioned  (we will be taking istio into account in this example and it is  installed in istio-system Namespace)&lt;&#x2F;li&gt;
&lt;li&gt;Meshery is installed in your Kubernetes cluster (Preferably in meshery namespace)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;see-in-action&quot;&gt;See in Action:&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;step-1-prerequisite-check&quot;&gt;Step 1 : Prerequisite check&lt;&#x2F;h4&gt;
&lt;p&gt;Lets see if everything mentioned in prerequisite is fulfilled&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code class=&quot;text-sm&quot;&gt;
$ kubectl get svc -n istio-system                                                           
NAME                   TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)                                      AGE
istio-ingressgateway   LoadBalancer   10.3.9.186   20.204.19.97    15021:30305&#x2F;TCP,80:32107&#x2F;TCP,443:32436&#x2F;TCP   37d
We have ingress gateway provisioned.
$ kubectl get po -n meshery                                                                 
NAME                                    READY   STATUS           RESTARTS      AGE
meshery-5cc4489f77-7sbc5                1&#x2F;1     Running             0          33d
meshery-operator-5db8b6c874-5cdvg       1&#x2F;1     Running             0          33d
meshery-meshsync-8lb8b6y784-6ghnk       1&#x2F;1     Running             0          33d
meshery-istio-6c56dd44fb-gk6xx          1&#x2F;1     Running             0          33d
$ kubectl get svc -n meshery                                                                
NAME                   TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)          AGE
meshery                LoadBalancer   10.3.9.178   10.5.3.188   9081:31037&#x2F;TCP   33d

&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now we see that Meshery‚Äôs core components meshery, meshery-operator, meshery-meshsync, meshery-istio (meshery adapter specific to your servicemesh), and meshery LoadBalance service (meshery) are up. The istio ingress gateway istio-ingressgateway is also up.&lt;&#x2F;p&gt;
&lt;p&gt;Note :&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;you can observe that the CLUSTER-IP &amp;amp; EXTERNAL-IP of the Meshery LoadBalancer are private IP (10.5.3.188, 10.3.9.178) that means you can not connect it from the browser or outside of the kubernetes cluster.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;in the other hand if you observe we have got an public ip for the istio-ingressgateway (20.204.19.97) which can be accessed from outside of the cluster.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Only way to access the Meshery is through our ingress gateway (which is part of ingress controller Istio in this case)&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;step-2-let-s-create-a-istio-gateway-for-this-meshery-which-will-facilitate-meshey-to-recive-the-request-from-outside-of-cluster-to-meshery&quot;&gt;Step 2: Let‚Äôs create a istio-Gateway for this  meshery, which will facilitate meshey to recive the request from outside  of cluster to Meshery.&lt;&#x2F;h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Tip:  What is istio-Gateway?
Gateway describes a load balancer operating at the edge of the mesh  receiving incoming or outgoing HTTP&#x2F;TCP connections. The specification  describes a set of ports that should be exposed, the type of protocol to  use, SNI configuration for the load balancer, etc. More information : Istio &#x2F; Gateway.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;create a file &lt;code&gt;meshery-istio-gw.yaml&lt;&#x2F;code&gt; and copy paste the below content to the file.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code class=&quot;text-sm&quot;&gt;
apiVersion: networking.istio.io&#x2F;v1alpha3
kind: Gateway
metadata:
  name: meshery-gateway
  namespace: meshery
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - &quot;*&quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now let‚Äôs apply this manifest in meshery Namespace.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code class=&quot;text-sm px-1&quot;&gt;
$ kubectl apply -f meshery-istio-gw.yaml
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Tip:
Istio gateway is namespace scoped.  The gateway listener for meshery listens on port 80.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h4 id=&quot;step-3-lets-create-a-virtualservice-for-istio-gateway-to-reach-meshery-loadbalancer-in-the-kubernetes-cluster&quot;&gt;Step 3: Lets create a virtualService for Istio gateway to reach Meshery Loadbalancer in the kubernetes cluster&lt;&#x2F;h4&gt;
&lt;blockquote&gt;
&lt;p&gt;üö© Tip: A VirtualService defines a set of traffic routing rules to apply when a  host is addressed. Each routing rule defines matching criteria for  traffic of a specific protocol. If the traffic is matched, then it is  sent to a named destination service (or subset&#x2F;version of it) defined in  the registry. More info: Istio &#x2F; Virtual Service.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;create a file &lt;code&gt;meshery-istio-vs.yaml&lt;&#x2F;code&gt; and copy paste the below content to the file.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code class=&quot;text-sm max-h-64&quot;&gt;
apiVersion: networking.istio.io&#x2F;v1alpha3
kind: VirtualService
metadata:
  name: meshery
  namespace: meshery
spec:
  hosts:
  - &quot;*&quot;
  gateways:
  - meshery-gateway
  http:
  - match:
    - uri:
        prefix: &#x2F;
    route:
    - destination:
        host: meshery
        port:
          number: 9081
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now let‚Äôs apply this manifest in meshery Namespace.&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code class=&quot;text-sm&quot;&gt;
$ kubectl apply -f meshery-istio-vs.yaml

&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Tip: istio virtualService is namespace scoped.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h4 id=&quot;understanding-meshery-istio-vs-yaml&quot;&gt;Understanding &lt;code&gt;meshery-istio-vs.yaml&lt;&#x2F;code&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;When the request hits the istio ingress gateway (the gateway listener for meshery listens on port 80) with prefix ‚Äú&#x2F;‚Äù (root) it will forward it to the kubernetes service meshery which exists in meshery Namespace and listing on 9081 (The default port of Meshery)&lt;&#x2F;p&gt;
&lt;h4 id=&quot;step-4-accessing-meshery&quot;&gt;Step 4: Accessing Meshery:&lt;&#x2F;h4&gt;
&lt;p&gt;Now you can access meshhery UI with http:&#x2F;&#x2F;&#x2F; in this context http:&#x2F;&#x2F;20.204.19.97&#x2F;
Tip: when you hit http:&#x2F;&#x2F;20.204.19.97&#x2F; it will automatically redirect you to
http:&#x2F;&#x2F;20.204.19.97&#x2F;provider and you will she the meshery UI as below:&lt;&#x2F;p&gt;
&lt;h4 id=&quot;extras&quot;&gt;Extras :&lt;&#x2F;h4&gt;
&lt;p&gt;You can use fqdn (let‚Äôs say meshery.example.com) for accessing meshery. In that case you have to replace &lt;code&gt;*&lt;&#x2F;code&gt; field under hosts: section in &lt;code&gt;meshery-istio-vs.yaml&lt;&#x2F;code&gt; and &lt;code&gt;meshery-istio-gw.yaml&lt;&#x2F;code&gt; to &lt;code&gt;meshery.example.com&lt;&#x2F;code&gt; .&lt;&#x2F;p&gt;
&lt;h4 id=&quot;find-it-on&quot;&gt;Find it on  : &lt;a href= &quot;https:&#x2F;&#x2F;discuss.layer5.io&#x2F;t&#x2F;configuring-meshery-behind-istio-ingress-gateway&#x2F;157&#x2F;&quot; target=&quot;_blank&quot;&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Layer5 discuss forum
&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;credits&quot;&gt;Credits:&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;layer5.io&#x2F;&quot; target=&quot;_blank&quot;&gt; Layer5&lt;&#x2F;a&gt; , &lt;a href=&quot;https:&#x2F;&#x2F;istio.io&#x2F;&quot; target=&quot;_blank&quot;&gt; Istio&lt;&#x2F;a&gt; , &lt;a href=&quot;https:&#x2F;&#x2F;meshery.io&#x2F;&quot; target=&quot;_blank&quot;&gt; Meshery&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
