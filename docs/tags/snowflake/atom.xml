<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>samirparhi.com - snowflake</title>
    <subtitle>Blog Site of Samir Parhi</subtitle>
    <link rel="self" type="application/atom+xml" href="https://samirparhi.com/tags/snowflake/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://samirparhi.com"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2024-04-12T00:00:00+00:00</updated>
    <id>https://samirparhi.com/tags/snowflake/atom.xml</id>
    <entry xml:lang="en">
        <title>Building a Dynamic Data Pipeline from Scratch with Snowflake and Python</title>
        <published>2024-04-12T00:00:00+00:00</published>
        <updated>2024-04-12T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Samir Ranjan Parhi
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://samirparhi.com/dependencies-tale/"/>
        <id>https://samirparhi.com/dependencies-tale/</id>
        
        <content type="html" xml:base="https://samirparhi.com/dependencies-tale/">&lt;h3 id=&quot;the-problem-statement&quot;&gt;The problem statement&lt;&#x2F;h3&gt;
&lt;p&gt;Sells 44 is an E-comerce platform which need some user data for it campaign. you are hence asked to extracts sales data from Snowflake, performs transformation using Pandas and NumPy (e.g., handling missing values and aggregating sales by region), and then saves the final output back to Snowflake or to a CSV for further analysis.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;system-design&quot;&gt;System Design&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;1-components&quot;&gt;1. Components&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Snowflake:&lt;&#x2F;em&gt; The cloud-based data warehouse where the raw sales data is stored.&lt;&#x2F;li&gt;
&lt;li&gt;Python: The programming language used for data pipeline orchestration.&lt;&#x2F;li&gt;
&lt;li&gt;Snowflake Connector for Python: This allows seamless interaction with Snowflake to extract and load data.&lt;&#x2F;li&gt;
&lt;li&gt;Pandas: Used for data manipulation, cleaning, and transformation.&lt;&#x2F;li&gt;
&lt;li&gt;NumPy: Used for numerical operations and advanced calculations.&lt;&#x2F;li&gt;
&lt;li&gt;CSV Output or Data Persistence: The final transformed data can be written back to Snowflake or exported as a CSV for external use.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;2-flow-the-etl&quot;&gt;2. Flow (The ETL)&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;Extract: Fetch raw sales data from Snowflake.&lt;&#x2F;li&gt;
&lt;li&gt;Transform: Clean the data using Pandas (handle missing values, filter, group, and calculate). Use NumPy for any complex numerical operations.&lt;&#x2F;li&gt;
&lt;li&gt;Load: Save the transformed data either back into Snowflake or export as a CSV for further reporting.&lt;&#x2F;li&gt;
&lt;li&gt;Establish a Snowflake Connection: Connects to Snowflake using your credentials and account information.&lt;&#x2F;li&gt;
&lt;li&gt;Extract: Runs a SQL query to fetch sales data.&lt;&#x2F;li&gt;
&lt;li&gt;Pandas Transformation:
&lt;ul&gt;
&lt;li&gt;Missing Values: Fills missing values in the sales_amount column with the mean.&lt;&#x2F;li&gt;
&lt;li&gt;NumPy Transformation: Applies a log transformation to scale the sales data using NumPy.&lt;&#x2F;li&gt;
&lt;li&gt;Aggregation: Groups the data by region and aggregates the total sales.&lt;&#x2F;li&gt;
&lt;li&gt;Load: Saves the transformed data to a CSV file or writes it back to Snowflake.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;lets-code-it&quot;&gt;Lets Code it&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;Setup the Snowflake Connector
You’ll need to install the Snowflake connector and Pandas library:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;code&gt;pip install snowflake-connector-python pandas numpy&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Code for the Data Pipeline:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre style=&quot;background-color:#212121;color:#eeffff;&quot;&gt;&lt;code&gt;&lt;span&gt;import snowflake.connector
&lt;&#x2F;span&gt;&lt;span&gt;import pandas as pd
&lt;&#x2F;span&gt;&lt;span&gt;import numpy as np
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Step 1: Establish connection to Snowflake
&lt;&#x2F;span&gt;&lt;span&gt;conn = snowflake.connector.connect(
&lt;&#x2F;span&gt;&lt;span&gt;    user=&amp;#39;your_username&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;    password=&amp;#39;your_password&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;    account=&amp;#39;your_account_url&amp;#39;
&lt;&#x2F;span&gt;&lt;span&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Step 2: Query Snowflake to extract data
&lt;&#x2F;span&gt;&lt;span&gt;query = &amp;quot;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;SELECT region, sales_amount, sales_date
&lt;&#x2F;span&gt;&lt;span&gt;FROM sales_data
&lt;&#x2F;span&gt;&lt;span&gt;WHERE sales_date &amp;gt;= &amp;#39;2024-01-01&amp;#39;
&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Step 3: Execute the query and fetch data
&lt;&#x2F;span&gt;&lt;span&gt;cur = conn.cursor()
&lt;&#x2F;span&gt;&lt;span&gt;cur.execute(query)
&lt;&#x2F;span&gt;&lt;span&gt;data = cur.fetchall()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Step 4: Load the data into a Pandas DataFrame
&lt;&#x2F;span&gt;&lt;span&gt;columns = [&amp;#39;region&amp;#39;, &amp;#39;sales_amount&amp;#39;, &amp;#39;sales_date&amp;#39;]
&lt;&#x2F;span&gt;&lt;span&gt;df = pd.DataFrame(data, columns=columns)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Step 5: Data Cleaning and Transformation
&lt;&#x2F;span&gt;&lt;span&gt;# Fill missing values in the &amp;#39;sales_amount&amp;#39; column with the mean
&lt;&#x2F;span&gt;&lt;span&gt;df[&amp;#39;sales_amount&amp;#39;].fillna(df[&amp;#39;sales_amount&amp;#39;].mean(), inplace=True)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Use NumPy to apply advanced numerical transformations (e.g., scaling)
&lt;&#x2F;span&gt;&lt;span&gt;df[&amp;#39;scaled_sales&amp;#39;] = np.log(df[&amp;#39;sales_amount&amp;#39;] + 1)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Aggregate sales by region
&lt;&#x2F;span&gt;&lt;span&gt;aggregated_data = df.groupby(&amp;#39;region&amp;#39;)[&amp;#39;sales_amount&amp;#39;].sum().reset_index()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Step 6: Save the transformed data back to Snowflake or to a CSV
&lt;&#x2F;span&gt;&lt;span&gt;# Save to CSV
&lt;&#x2F;span&gt;&lt;span&gt;aggregated_data.to_csv(&amp;#39;aggregated_sales_data.csv&amp;#39;, index=False)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Alternatively, you can write the data back to Snowflake:
&lt;&#x2F;span&gt;&lt;span&gt;# cur.execute(&amp;quot;CREATE OR REPLACE TABLE transformed_sales_data (region STRING, total_sales FLOAT)&amp;quot;)
&lt;&#x2F;span&gt;&lt;span&gt;# for index, row in aggregated_data.iterrows():
&lt;&#x2F;span&gt;&lt;span&gt;#     cur.execute(f&amp;quot;INSERT INTO transformed_sales_data (region, total_sales) VALUES (&amp;#39;{row[&amp;#39;region&amp;#39;]}&amp;#39;, {row[&amp;#39;sales_amount&amp;#39;]})&amp;quot;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Close the cursor and connection
&lt;&#x2F;span&gt;&lt;span&gt;cur.close()
&lt;&#x2F;span&gt;&lt;span&gt;conn.close()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;print(&amp;quot;Data pipeline completed successfully!&amp;quot;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;scalability&quot;&gt;Scalability:&lt;&#x2F;h3&gt;
&lt;p&gt;The architecture allows easy scaling as Snowflake handles large datasets efficiently.
Python’s flexibility enables more complex transformations if needed.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;deployment-considerations&quot;&gt;Deployment Considerations:&lt;&#x2F;h3&gt;
&lt;p&gt;The pipeline can be scheduled using Jenkins jobs or GitHub Action&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#212121;color:#eeffff;&quot;&gt;&lt;code&gt;&lt;span&gt;name: Snowflake Pipeline Schedule
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;# Schedule to run daily at 12:00 AM (UTC)
&lt;&#x2F;span&gt;&lt;span&gt;on:
&lt;&#x2F;span&gt;&lt;span&gt;  schedule:
&lt;&#x2F;span&gt;&lt;span&gt;    - cron: &amp;#39;0 0 * * *&amp;#39;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  workflow_dispatch: # Allows manual trigger
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;jobs:
&lt;&#x2F;span&gt;&lt;span&gt;  run-snowflake-pipeline:
&lt;&#x2F;span&gt;&lt;span&gt;    runs-on: ubuntu-latest
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    steps:
&lt;&#x2F;span&gt;&lt;span&gt;    # Step 1: Checkout the repository code
&lt;&#x2F;span&gt;&lt;span&gt;    - name: Checkout code
&lt;&#x2F;span&gt;&lt;span&gt;      uses: actions&#x2F;checkout@v3
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    # Step 2: Set up Python environment
&lt;&#x2F;span&gt;&lt;span&gt;    - name: Set up Python 3.x
&lt;&#x2F;span&gt;&lt;span&gt;      uses: actions&#x2F;setup-python@v4
&lt;&#x2F;span&gt;&lt;span&gt;      with:
&lt;&#x2F;span&gt;&lt;span&gt;        python-version: &amp;#39;3.x&amp;#39;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    # Step 3: Install dependencies
&lt;&#x2F;span&gt;&lt;span&gt;    - name: Install dependencies
&lt;&#x2F;span&gt;&lt;span&gt;      run: |
&lt;&#x2F;span&gt;&lt;span&gt;        python -m pip install --upgrade pip
&lt;&#x2F;span&gt;&lt;span&gt;        pip install snowflake-connector-python pandas numpy
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    # Step 4: Run the Snowflake pipeline
&lt;&#x2F;span&gt;&lt;span&gt;    - name: Run Snowflake Data Pipeline
&lt;&#x2F;span&gt;&lt;span&gt;      env:
&lt;&#x2F;span&gt;&lt;span&gt;        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
&lt;&#x2F;span&gt;&lt;span&gt;        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
&lt;&#x2F;span&gt;&lt;span&gt;        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
&lt;&#x2F;span&gt;&lt;span&gt;      run: |
&lt;&#x2F;span&gt;&lt;span&gt;        python snowflake_pipeline.py
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
</content>
        
    </entry>
</feed>
